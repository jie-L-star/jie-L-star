{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7348091,"sourceType":"datasetVersion","datasetId":4266878},{"sourceId":7348441,"sourceType":"datasetVersion","datasetId":4267123}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/uniquejie/notebookd1?scriptVersionId=158267912\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os\nos.chdir('/kaggle/input/learning-code')\n!pip install mat73\n!pip install github\n!pip install pygithub","metadata":{"execution":{"iopub.status.busy":"2024-01-09T07:11:39.504322Z","iopub.execute_input":"2024-01-09T07:11:39.505329Z","iopub.status.idle":"2024-01-09T07:11:39.512595Z","shell.execute_reply.started":"2024-01-09T07:11:39.505289Z","shell.execute_reply":"2024-01-09T07:11:39.511948Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import random\nfrom datetime import datetime\n\nimport mat73\nimport torch\n\nfrom MODEL import *     # 从模型文件中加载定义的网络架构\nSeedNumber = 42\nrandom.seed(SeedNumber)\nnp.random.seed(SeedNumber)\ntorch.manual_seed(SeedNumber)\n# 训练前准备，检测是否有可用的GPU，有则使用\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# data_all\ndataMat = mat73.loadmat('/kaggle/input/rader-data/data_all.mat')\nsignal_train = torch.from_numpy(dataMat['train_signal'])\nsignal_test = torch.from_numpy(dataMat['test_signal'])\n\nlabel_train_cd = dataMat['train_cd']\nlabel_train_JW = dataMat['ltrain_JW']\nlabel_test_cd = dataMat['test_cd']\nlabel_test_JW = dataMat['ltest_JW']\n\n# data_200\n# dataMat = mat73.loadmat('D:/Pycharm/master/deep_learning/code4/data_200.mat')\n# signal_train = torch.from_numpy(dataMat['train_signal_200'])\n# signal_test = torch.from_numpy(dataMat['test_signal_200'])\n#\n# label_train_cd = dataMat['train_cd_200']\n# label_train_JW = dataMat['train_label_200']\n# label_test_cd = dataMat['test_cd_200']\n# label_test_JW = dataMat['test_label_200']\n\nLabel_train = torch.from_numpy(label_train_cd)\ndH_train = label_train_JW[:, 2]\n\nLabel_test = torch.from_numpy(label_test_cd)\ndH_test = label_test_JW[:, 2]\n\nArray_num = signal_train.shape[0]\nSnap_num = signal_train.shape[1]\nSample_train = signal_train.shape[2]\nSample_test = signal_test.shape[2]\n\nSnap_len = 2000\n\nH = np.zeros((Array_num, Snap_num), dtype=complex)\nX_train = np.zeros((Sample_train, 4, Array_num, Array_num), dtype=float)\nfor Sample_index in range(1, Sample_train + 1):\n    H = signal_train[:, 0:Snap_len, Sample_index-1]\n    R_temp = 1 / Snap_len * torch.mm(H, torch.adjoint(H))\n\n    X_train[Sample_index - 1, 0, :, :] = torch.real(R_temp)\n    X_train[Sample_index - 1, 1, :, :] = torch.imag(R_temp)\n    X_train[Sample_index - 1, 2, :, :] = torch.angle(R_temp)\n    X_train[Sample_index - 1, 3, :, :] = torch.ones_like(R_temp) * dH_train[Sample_index-1]\n\nX_test = np.zeros((Sample_test, 4, Array_num, Array_num), dtype=float)\nfor Sample_index in range(1, Sample_test + 1):\n    H = signal_test[:, 0:Snap_len, Sample_index-1]\n    R_temp = 1 / Snap_len * torch.mm(H, torch.adjoint(H))\n\n    X_test[Sample_index - 1, 0, :, :] = torch.real(R_temp)\n    X_test[Sample_index - 1, 1, :, :] = torch.imag(R_temp)\n    X_test[Sample_index - 1, 2, :, :] = torch.angle(R_temp)\n    X_test[Sample_index - 1, 3, :, :] = torch.ones_like(R_temp) * dH_test[Sample_index-1]\n\n\n# X[:, Array_num*Array_num] = dH_train\n\n# X_train, X_test, Label_train, Label_test = train_test_split(X, label_train, test_size=0.01)\n\n# scales = MinMaxScaler(feature_range=(0, 1))\n# X_train_s = scales.fit_transform(X_train)\n# X_test_s = scales.fit_transform(X_test)   # 对数据归一化处理\n\n# 将数据转为张量\n\nX_train_nots = torch.from_numpy(X_train.astype(np.float64)).to(device=device)\n\nLabel_train_t = Label_train.to(device=device)\n\nX_test_nots = torch.from_numpy(X_test.astype(np.float64)).to(device=device)\n\nLabel_test_t = Label_test.to(device=device)\n\n# 将训练集转化为张量后，使用TensorDataset将输入和标签整理到一起\ntrain_data_nots = TensorDataset(X_train_nots, Label_train_t)\ntest_data_nots = TensorDataset(X_test_nots, Label_test_t)\n\n##\n# 选取网络，定义网络超参数\ninput_dim = 64\nhidden_dim = 20\nhidden_layer = 10\nout_dim = 1\n\n# Model = LSTM3(input_dim, hidden_dim, hidden_layer, out_dim)   # 选取网络，括号中为输入信号的维度\nModel = cnn3()\n# Model = net6()\nModel.to(device)\n\n# 网络超参数\n# 32->64\ntrain_batch_size = 32\ntest_batch_size = 32\nlr = 0.01\nnum_epoch = 5\nmomentum = 0.8\nMinTrainLoss = 1e8\n\n# 定义数据加载器，将训练集进行批处理\ntrain_nots_loader = DataLoader(\n    dataset=train_data_nots,\n    batch_size=train_batch_size,\n    shuffle=True,\n    drop_last=True\n    )\ntest_nots_loader = DataLoader(\n    dataset=test_data_nots,\n    batch_size=test_batch_size,\n    shuffle=True,\n    drop_last=True\n)\n\n# 定义损失函数和优化器\nLoss = nn.MSELoss()\noptimizer = torch.optim.Adam(Model.parameters(), lr=lr)\n\n##\n# 训练模型\n\nlosses_J = []\nlosses_W = []\nacces = []\neval_losses_J = []\neval_losses_W = []\neval_acces = []\nlosses_all = []\n\nbegin = datetime.now().timestamp()\n\n# 写入日志文件\nf = open('/kaggle/working/train_log.txt', 'w')\nfor epoch in range(num_epoch):\n    train_loss_J = 0\n    train_loss_W = 0\n    # loss1 = 0\n    # loss2 = 0\n    losses = 0\n    train_acc = 0\n    Model.train()      # 开始训练\n# 动态修改学习率\n    if epoch <= 5000 and epoch % 1000 == 0:\n        optimizer.param_groups[0]['lr'] *= 0.1\n    for step, (b_x, b_y) in enumerate(train_nots_loader):\n        # 前向传播\n        out = Model(b_x)\n        # out = ar2JW(out, b_x[:, 3, 1, 1])\n        loss1 = Loss(out[:, 0], b_y[:, 0])\n        loss2 = Loss(out[:, 1], b_y[:, 1])\n        # loss = (loss1 + loss2)\n        loss = loss1 + loss2\n        # 反向传播\n        optimizer.zero_grad()\n        #\n        # loss.backward()\n        loss.backward()\n        # loss2.backward()\n        optimizer.step()\n        # 记录误差\n        train_loss_J += loss1.item()\n        train_loss_W += loss2.item()\n        losses += loss.item()\n\n        # # 计算分类的准确性\n        # _, pred = out.max(1)\n        # num_correct = (pred == b_y).sum().item()\n        # acc = num_correct / step\n        # train_acc += acc\n\n    losses_J.append(train_loss_J / len(train_nots_loader))\n    losses_W.append(train_loss_W / len(train_nots_loader))\n    losses_all.append(losses)\n    # acces.append(train_acc / len((train_nots_loader)))\n    \n    # 在测试集上验证效果\n    eval_loss_J = 0\n    eval_loss_W = 0\n    eval_acc = 0\n    # 模型状态改为测试\n    Model.eval()\n    for step, (b_x, b_y) in enumerate(test_nots_loader):\n        out = Model(b_x)\n        # out = ar2JW(out, b_x[:, 3, 1, 1])\n        loss1 = Loss(out[:, 0], b_y[:, 0])\n        loss2 = Loss(out[:, 1], b_y[:, 1])\n        # 记录误差\n        eval_loss_J += loss1.item()\n        eval_loss_W += loss2.item()\n        # # 记录准确率\n        # _, pred = out.max(1)\n        # num_correct = (pred == b_y).sum().item()\n        # acc = num_correct / step\n        # eval_acc +=acc\n    eval_losses_J.append(eval_loss_J / len(test_nots_loader))\n    eval_losses_W.append(eval_loss_W / len(test_nots_loader))\n    # eval_acces.append(eval_acc / len(test_nots_loader))\n\n    # 获取时间戳\n    str = 'epoch: {}, Train Loss J : {:.6f}, Train Loss W : {:.6f}, Test Loss J : {:.6f}, Test Loss W : {:.6f}'.format\\\n          (epoch, train_loss_J / len(train_nots_loader), train_loss_W / len(train_nots_loader),\n           eval_loss_J / len(test_nots_loader), eval_loss_W / len(test_nots_loader))\n    print(str + '  当前时间戳：{:.1f}'.format(datetime.now().timestamp()-begin))\n    print(str, file=f)\n    ##\n    # 保存模型\n    if losses_all[-1] < MinTrainLoss:\n        torch.save(Model.state_dict(), \"/kaggle/working/model_iter.pth\")  # 保存每一次loss下降的模型\n        MinTrainLoss = losses_all[-1]\n\n# 可视化训练及测试损失值\nf.close()\n\nplt.title('trainloss')\nplt.plot(np.arange(len(losses_J)), losses_J)\nplt.plot(np.arange(len(losses_W)), losses_W)\nplt.plot(np.arange(len(eval_losses_J)), eval_losses_J)\nplt.plot(np.arange(len(eval_losses_W)), eval_losses_W)\nplt.legend(['Train Loss J', 'Train Loss W', 'Test Loss J', 'Test Loss W'], loc='upper right')\n# plt.legend(['Test Loss'], loc='upper right')\nplt.savefig('/kaggle/working/Test Loss.jpg')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-09T07:11:39.525442Z","iopub.execute_input":"2024-01-09T07:11:39.525719Z","iopub.status.idle":"2024-01-09T07:20:15.290684Z","shell.execute_reply.started":"2024-01-09T07:11:39.52569Z","shell.execute_reply":"2024-01-09T07:20:15.289453Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/torch/_tensor.py:1032: ComplexWarning: Casting complex values to real discards the imaginary part\n  return self.numpy().astype(dtype, copy=False)\n","output_type":"stream"},{"name":"stdout","text":"epoch: 0, Train Loss J : 45.819799, Train Loss W : 91.379343, Test Loss J : 37.361884, Test Loss W : 2.634280  当前时间戳：8.4\nepoch: 1, Train Loss J : 38.195941, Train Loss W : 19.147185, Test Loss J : 37.876326, Test Loss W : 2.935611  当前时间戳：16.9\nepoch: 2, Train Loss J : 38.638461, Train Loss W : 17.578632, Test Loss J : 37.792516, Test Loss W : 2.472593  当前时间戳：25.4\nepoch: 3, Train Loss J : 37.850588, Train Loss W : 17.506799, Test Loss J : 36.185887, Test Loss W : 1.924247  当前时间戳：34.0\nepoch: 4, Train Loss J : 37.339961, Train Loss W : 16.233290, Test Loss J : 35.754521, Test Loss W : 6.691531  当前时间戳：42.6\nepoch: 5, Train Loss J : 36.138630, Train Loss W : 17.425599, Test Loss J : 34.033724, Test Loss W : 1.448749  当前时间戳：51.1\nepoch: 6, Train Loss J : 36.662517, Train Loss W : 17.238285, Test Loss J : 34.034511, Test Loss W : 44.566118  当前时间戳：59.4\nepoch: 7, Train Loss J : 35.200804, Train Loss W : 17.050790, Test Loss J : 32.767016, Test Loss W : 3.232783  当前时间戳：67.7\nepoch: 8, Train Loss J : 34.081503, Train Loss W : 16.452990, Test Loss J : 34.180357, Test Loss W : 5.193439  当前时间戳：76.1\nepoch: 9, Train Loss J : 33.141632, Train Loss W : 16.745744, Test Loss J : 33.232518, Test Loss W : 4.924628  当前时间戳：84.6\nepoch: 10, Train Loss J : 33.030399, Train Loss W : 16.636234, Test Loss J : 30.902453, Test Loss W : 6.005834  当前时间戳：92.9\nepoch: 11, Train Loss J : 31.431455, Train Loss W : 15.809846, Test Loss J : 29.875148, Test Loss W : 1.617813  当前时间戳：101.4\nepoch: 12, Train Loss J : 31.717341, Train Loss W : 16.561266, Test Loss J : 26.903716, Test Loss W : 1.289988  当前时间戳：109.9\nepoch: 13, Train Loss J : 31.031744, Train Loss W : 15.026931, Test Loss J : 27.294978, Test Loss W : 5.896345  当前时间戳：118.3\nepoch: 14, Train Loss J : 30.082973, Train Loss W : 16.894200, Test Loss J : 26.609054, Test Loss W : 10.713021  当前时间戳：126.6\nepoch: 15, Train Loss J : 29.472160, Train Loss W : 15.102082, Test Loss J : 27.348336, Test Loss W : 3.047918  当前时间戳：134.9\nepoch: 16, Train Loss J : 29.966041, Train Loss W : 18.196674, Test Loss J : 27.253706, Test Loss W : 1.220100  当前时间戳：143.4\nepoch: 17, Train Loss J : 30.015767, Train Loss W : 14.467759, Test Loss J : 26.507819, Test Loss W : 3.409937  当前时间戳：151.7\nepoch: 18, Train Loss J : 29.010989, Train Loss W : 15.121878, Test Loss J : 27.987289, Test Loss W : 10.835532  当前时间戳：160.0\nepoch: 19, Train Loss J : 28.879914, Train Loss W : 16.034778, Test Loss J : 27.191293, Test Loss W : 2.146701  当前时间戳：168.4\nepoch: 20, Train Loss J : 29.466545, Train Loss W : 16.166548, Test Loss J : 25.898576, Test Loss W : 1.975856  当前时间戳：176.7\nepoch: 21, Train Loss J : 28.754662, Train Loss W : 14.214314, Test Loss J : 25.762697, Test Loss W : 2.506189  当前时间戳：185.2\nepoch: 22, Train Loss J : 28.479272, Train Loss W : 15.133654, Test Loss J : 25.797887, Test Loss W : 10.797607  当前时间戳：193.6\nepoch: 23, Train Loss J : 28.638293, Train Loss W : 13.714938, Test Loss J : 25.398440, Test Loss W : 2.944121  当前时间戳：201.8\nepoch: 24, Train Loss J : 28.419898, Train Loss W : 13.245331, Test Loss J : 24.999826, Test Loss W : 1.070687  当前时间戳：210.1\nepoch: 25, Train Loss J : 27.953804, Train Loss W : 16.219478, Test Loss J : 26.449403, Test Loss W : 1.417293  当前时间戳：218.6\nepoch: 26, Train Loss J : 27.813995, Train Loss W : 15.624724, Test Loss J : 24.898866, Test Loss W : 1.430012  当前时间戳：226.9\nepoch: 27, Train Loss J : 28.168925, Train Loss W : 18.060613, Test Loss J : 25.035198, Test Loss W : 11.973351  当前时间戳：235.1\nepoch: 28, Train Loss J : 28.415631, Train Loss W : 16.803298, Test Loss J : 25.862172, Test Loss W : 1.214918  当前时间戳：243.5\nepoch: 29, Train Loss J : 27.843695, Train Loss W : 13.319119, Test Loss J : 24.848393, Test Loss W : 1.085419  当前时间戳：251.8\nepoch: 30, Train Loss J : 28.495473, Train Loss W : 13.842741, Test Loss J : 25.184177, Test Loss W : 1.653827  当前时间戳：260.3\nepoch: 31, Train Loss J : 27.774890, Train Loss W : 12.929277, Test Loss J : 25.374893, Test Loss W : 3.400645  当前时间戳：268.6\nepoch: 32, Train Loss J : 27.513059, Train Loss W : 13.446379, Test Loss J : 25.791346, Test Loss W : 1.180240  当前时间戳：277.1\nepoch: 33, Train Loss J : 27.907022, Train Loss W : 14.626269, Test Loss J : 25.445016, Test Loss W : 1.604920  当前时间戳：285.5\nepoch: 34, Train Loss J : 27.388191, Train Loss W : 13.481612, Test Loss J : 24.092101, Test Loss W : 3.515342  当前时间戳：293.8\nepoch: 35, Train Loss J : 27.716071, Train Loss W : 12.948574, Test Loss J : 25.452075, Test Loss W : 9.259697  当前时间戳：302.3\nepoch: 36, Train Loss J : 27.904418, Train Loss W : 13.628827, Test Loss J : 24.643906, Test Loss W : 2.596770  当前时间戳：310.8\nepoch: 37, Train Loss J : 27.429528, Train Loss W : 15.236356, Test Loss J : 25.138195, Test Loss W : 11.831250  当前时间戳：319.1\nepoch: 38, Train Loss J : 26.785745, Train Loss W : 13.255296, Test Loss J : 23.949330, Test Loss W : 1.126763  当前时间戳：327.6\nepoch: 39, Train Loss J : 27.119131, Train Loss W : 13.920672, Test Loss J : 24.671719, Test Loss W : 4.900241  当前时间戳：336.1\nepoch: 40, Train Loss J : 26.951027, Train Loss W : 14.444982, Test Loss J : 25.736109, Test Loss W : 0.729853  当前时间戳：344.4\nepoch: 41, Train Loss J : 26.832288, Train Loss W : 13.970415, Test Loss J : 24.618445, Test Loss W : 1.988111  当前时间戳：352.7\nepoch: 42, Train Loss J : 27.067008, Train Loss W : 12.804143, Test Loss J : 24.041490, Test Loss W : 1.359777  当前时间戳：361.0\nepoch: 43, Train Loss J : 27.217920, Train Loss W : 12.616775, Test Loss J : 23.756323, Test Loss W : 0.954953  当前时间戳：369.5\nepoch: 44, Train Loss J : 26.703662, Train Loss W : 17.219682, Test Loss J : 24.293155, Test Loss W : 10.132717  当前时间戳：377.8\nepoch: 45, Train Loss J : 26.228206, Train Loss W : 12.530970, Test Loss J : 23.877020, Test Loss W : 1.829900  当前时间戳：386.2\nepoch: 46, Train Loss J : 26.455249, Train Loss W : 12.161485, Test Loss J : 22.872487, Test Loss W : 1.018652  当前时间戳：394.8\nepoch: 47, Train Loss J : 26.373469, Train Loss W : 12.363944, Test Loss J : 24.789027, Test Loss W : 10.368397  当前时间戳：403.3\nepoch: 48, Train Loss J : 26.293438, Train Loss W : 13.562902, Test Loss J : 24.429074, Test Loss W : 2.713704  当前时间戳：411.5\nepoch: 49, Train Loss J : 26.973399, Train Loss W : 14.368628, Test Loss J : 23.753165, Test Loss W : 0.825461  当前时间戳：419.7\nepoch: 50, Train Loss J : 26.586498, Train Loss W : 14.528501, Test Loss J : 25.263546, Test Loss W : 0.682247  当前时间戳：427.9\nepoch: 51, Train Loss J : 26.125016, Train Loss W : 12.706675, Test Loss J : 22.612706, Test Loss W : 9.876819  当前时间戳：436.1\nepoch: 52, Train Loss J : 26.093754, Train Loss W : 11.951213, Test Loss J : 23.622964, Test Loss W : 1.667153  当前时间戳：444.3\nepoch: 53, Train Loss J : 26.068187, Train Loss W : 13.513642, Test Loss J : 22.727845, Test Loss W : 4.137638  当前时间戳：452.7\nepoch: 54, Train Loss J : 26.164020, Train Loss W : 12.786266, Test Loss J : 25.393969, Test Loss W : 1.139566  当前时间戳：461.0\nepoch: 55, Train Loss J : 26.660592, Train Loss W : 12.510695, Test Loss J : 24.611211, Test Loss W : 1.688508  当前时间戳：469.5\nepoch: 56, Train Loss J : 26.218441, Train Loss W : 12.038038, Test Loss J : 23.093415, Test Loss W : 1.212729  当前时间戳：477.7\nepoch: 57, Train Loss J : 25.118993, Train Loss W : 11.548977, Test Loss J : 24.373757, Test Loss W : 1.020068  当前时间戳：486.0\nepoch: 58, Train Loss J : 26.838722, Train Loss W : 13.298563, Test Loss J : 22.909997, Test Loss W : 1.693797  当前时间戳：494.3\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 159\u001b[0m\n\u001b[1;32m    156\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (b_x, b_y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_nots_loader):\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# out = ar2JW(out, b_x[:, 3, 1, 1])\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     loss1 \u001b[38;5;241m=\u001b[39m Loss(out[:, \u001b[38;5;241m0\u001b[39m], b_y[:, \u001b[38;5;241m0\u001b[39m])\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/kaggle/input/learning-code/MODEL.py:755\u001b[0m, in \u001b[0;36mcnn3.forward\u001b[0;34m(self, dx)\u001b[0m\n\u001b[1;32m    753\u001b[0m x \u001b[38;5;241m=\u001b[39m dx[:, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m, :, :]\n\u001b[1;32m    754\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mS(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[0;32m--> 755\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mS(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    757\u001b[0m \u001b[38;5;66;03m# x41 = self.lstm(x3)\u001b[39;00m\n\u001b[1;32m    758\u001b[0m x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mS(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x2)))\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"from github import Github\nimport os\n\ntoken = \"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\"\ng = Github(token)\n\nrepo_name = \"kaggle_output\"\nrepo = g.get_user().get_repo(repo_name)\n\n# 定义文件夹和文件类型的映射关系\nfolder_mapping = {\n    \".txt\": \"log_files\",\n    \".pth\": \"model_files\",\n    \".jpg\": \"los_image_files\"\n}\n\nlocal_folder = \"/kaggle/working/\"\nfor file_name in os.listdir(local_folder):\n    if os.path.isfile(os.path.join(local_folder, file_name)):\n        file_extension = os.path.splitext(file_name)[1].lower()\n\n        # 确保文件类型在映射关系中\n        if file_extension in folder_mapping:\n            folder_name = folder_mapping[file_extension]\n\n            # 获取目标文件夹中现有文件的数量\n            existing_files = repo.get_contents(folder_name)\n            num_files = len(existing_files)\n\n            # 构建新文件的名称\n            new_file_name = f\"{file_name.split('.')[0]}{num_files}{file_extension}\"\n            remote_file_path = f\"{folder_name}/{new_file_name}\"\n\n            # 读取本地文件的内容\n            with open(os.path.join(local_folder, file_name), 'rb') as file:\n                file_content = file.read()\n\n            # 在仓库中创建或更新文件\n            repo.create_file(remote_file_path, f\"Upload {remote_file_path}\", file_content, branch=\"main\")\n\nprint(\"Files uploaded successfully.\")","metadata":{"execution":{"iopub.status.busy":"2024-01-09T07:23:30.912536Z","iopub.execute_input":"2024-01-09T07:23:30.913351Z","iopub.status.idle":"2024-01-09T07:23:38.139976Z","shell.execute_reply.started":"2024-01-09T07:23:30.913315Z","shell.execute_reply":"2024-01-09T07:23:38.139305Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Files uploaded successfully.\n","output_type":"stream"}]}]}